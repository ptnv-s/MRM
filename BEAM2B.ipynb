{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BEAM2B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kxfdP4hJUPB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ebc550-5284-4091-dc74-3246773890f4"
      },
      "source": [
        "!pip install tensorflow-addons==0.11.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-addons==0.11.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/9b/198d3745f937d1ab244eabc4ffdc420f84f2b86d6d8f0937f3f0acc1b258/tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 20.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 28.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 21.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 25.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 26.3MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 29.0MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 19.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 21.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 19.4MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 19.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 19.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 19.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 19.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 19.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153kB 19.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911kB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942kB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0MB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0MB 19.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0MB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0MB 19.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1MB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1MB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1MB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1MB 19.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1MB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnxXKDjq3jEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60cbb6d0-942c-4b18-aa7e-3d92aa539c20"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvRnGWnvXm6l"
      },
      "source": [
        "def download_nmt():\n",
        "    path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "    path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\"\n",
        "    return path_to_file\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMAHz7kJXc5N"
      },
      "source": [
        "class NMTDataset:\n",
        "    def __init__(self, problem_type='en-spa'):\n",
        "        self.problem_type = 'en-spa'\n",
        "        self.inp_lang_tokenizer = None\n",
        "        self.targ_lang_tokenizer = None\n",
        "    \n",
        "\n",
        "    def unicode_to_ascii(self, s):\n",
        "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "    ## Step 1 and Step 2 \n",
        "    def preprocess_sentence(self, w):\n",
        "        w = self.unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "        # creating a space between a word and the punctuation following it\n",
        "        # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "        w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "        w = w.strip()\n",
        "\n",
        "        # adding a start and an end token to the sentence\n",
        "        # so that the model know when to start and stop predicting.\n",
        "        w = '<start> ' + w + ' <end>'\n",
        "        return w\n",
        "    \n",
        "    def create_dataset(self, path, num_examples):\n",
        "        # path : path to spa-eng.txt file\n",
        "        # num_examples : Limit the total number of training example for faster training (set num_examples = len(lines) to use full data)\n",
        "        lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "        word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "\n",
        "        return zip(*word_pairs)\n",
        "\n",
        "    # Step 3 and Step 4\n",
        "    def tokenize(self, lang):\n",
        "        # lang = list of sentences in a language\n",
        "        \n",
        "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
        "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
        "        lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "\n",
        "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "        ## and pads the sequences to match the longest sequences in the given input\n",
        "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "        return tensor, lang_tokenizer\n",
        "\n",
        "    def load_dataset(self, path, num_examples=None):\n",
        "        # creating cleaned input, output pairs\n",
        "        targ_lang, inp_lang = self.create_dataset(path, num_examples)\n",
        "\n",
        "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
        "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
        "\n",
        "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
        "\n",
        "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
        "        file_path = download_nmt()\n",
        "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)\n",
        "        \n",
        "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
        "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
        "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIW4NVBmJ25k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc19de54-8d56-4498-fe3b-9fd3a31890d9"
      },
      "source": [
        "BUFFER_SIZE = 32000\n",
        "BATCH_SIZE = 64\n",
        "# Let's limit the #training examples for faster training\n",
        "num_examples = 30000\n",
        "\n",
        "dataset_creator = NMTDataset('en-spa')\n",
        "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2lCTy4vKOkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ddf88e-5652-4824-c824-482a310ee0a3"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 16]), TensorShape([64, 11]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqHsArVZ3jFS"
      },
      "source": [
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "max_length_input = example_input_batch.shape[1]\n",
        "max_length_output = example_target_batch.shape[1]\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "steps_per_epoch = num_examples//BATCH_SIZE\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-yY9c6aIu1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8403c20e-07fb-4fa1-e1a3-a00f38973a34"
      },
      "source": [
        "print(\"max_length_spanish, max_length_english, vocab_size_spanish, vocab_size_english\")\n",
        "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_length_spanish, max_length_english, vocab_size_spanish, vocab_size_english\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 11, 9415, 4936)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ2rI24i3jFg"
      },
      "source": [
        "##### \n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    ##-------- LSTM layer in Encoder ------- ##\n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    \n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
        "    return output, h, c\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))] "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60gSVh05Jl6l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d66408-9d42-4b99-d7a9-84931dc31871"
      },
      "source": [
        "## Test Encoder Stack\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
            "Encoder h vecotr shape: (batch size, units) (64, 1024)\n",
            "Encoder c vector shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_B3mhW3jFk"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention_type = attention_type\n",
        "    \n",
        "    # Embedding Layer\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    \n",
        "    #Final Dense layer on which softmax will be applied\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "   \n",
        "\n",
        "\n",
        "    # Sampler\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "    # Create attention mechanism with memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
        "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
        "\n",
        "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "\n",
        "    # Define the decoder with respect to fundamental rnn cell\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "    \n",
        "  def build_rnn_cell(self, batch_sz):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
        "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
        "    return outputs\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaiO0Z6_Ml1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "601642c7-278c-4216-858f-4e4d8dc71b55"
      },
      "source": [
        "# Test decoder stack\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'bahdanau')\n",
        "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
        "decoder.attention_mechanism.setup_memory(sample_output)\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
        "\n",
        "\n",
        "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
        "\n",
        "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder Outputs Shape:  (64, 10, 4936)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTHr5iV3jFr"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adadelta()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # real shape = (BATCH_SIZE, max_length_output)\n",
        "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss  "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj8bXQTgNwrF"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9ArXSsVfqn"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\n",
        "\n",
        "    # Set the AttentionMechanism object with encoder_outputs\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    # Create AttentionWrapperState as initial_state for decoder\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "    pred = decoder(dec_input, decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_function(real, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddefjBMa3jF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adac88fe-f4e9-4873-f4d5-3e5f99ebb6ca"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.1963\n",
            "Epoch 1 Batch 100 Loss 4.9829\n",
            "Epoch 1 Batch 200 Loss 4.9690\n",
            "Epoch 1 Batch 300 Loss 5.2605\n",
            "Epoch 1 Loss 4.0790\n",
            "Time taken for 1 epoch 32.79840660095215 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.4726\n",
            "Epoch 2 Batch 100 Loss 4.9670\n",
            "Epoch 2 Batch 200 Loss 5.0061\n",
            "Epoch 2 Batch 300 Loss 4.8328\n",
            "Epoch 2 Loss 4.0769\n",
            "Time taken for 1 epoch 25.900840044021606 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.8721\n",
            "Epoch 3 Batch 100 Loss 5.1901\n",
            "Epoch 3 Batch 200 Loss 5.2156\n",
            "Epoch 3 Batch 300 Loss 5.1618\n",
            "Epoch 3 Loss 4.0746\n",
            "Time taken for 1 epoch 24.792736530303955 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 5.2539\n",
            "Epoch 4 Batch 100 Loss 5.3198\n",
            "Epoch 4 Batch 200 Loss 5.0665\n",
            "Epoch 4 Batch 300 Loss 5.1453\n",
            "Epoch 4 Loss 4.0719\n",
            "Time taken for 1 epoch 25.283175706863403 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 5.1312\n",
            "Epoch 5 Batch 100 Loss 5.1299\n",
            "Epoch 5 Batch 200 Loss 5.1287\n",
            "Epoch 5 Batch 300 Loss 5.2873\n",
            "Epoch 5 Loss 4.0689\n",
            "Time taken for 1 epoch 24.778567790985107 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 5.1671\n",
            "Epoch 6 Batch 100 Loss 5.0594\n",
            "Epoch 6 Batch 200 Loss 5.1377\n",
            "Epoch 6 Batch 300 Loss 4.9644\n",
            "Epoch 6 Loss 4.0655\n",
            "Time taken for 1 epoch 25.526586532592773 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 5.1091\n",
            "Epoch 7 Batch 100 Loss 5.1480\n",
            "Epoch 7 Batch 200 Loss 5.0664\n",
            "Epoch 7 Batch 300 Loss 4.9324\n",
            "Epoch 7 Loss 4.0615\n",
            "Time taken for 1 epoch 24.758620262145996 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 5.0241\n",
            "Epoch 8 Batch 100 Loss 4.9024\n",
            "Epoch 8 Batch 200 Loss 5.1401\n",
            "Epoch 8 Batch 300 Loss 5.0710\n",
            "Epoch 8 Loss 4.0566\n",
            "Time taken for 1 epoch 25.311031103134155 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.8712\n",
            "Epoch 9 Batch 100 Loss 5.1737\n",
            "Epoch 9 Batch 200 Loss 5.2779\n",
            "Epoch 9 Batch 300 Loss 5.0241\n",
            "Epoch 9 Loss 4.0505\n",
            "Time taken for 1 epoch 24.75413727760315 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 4.9159\n",
            "Epoch 10 Batch 100 Loss 4.8739\n",
            "Epoch 10 Batch 200 Loss 5.3475\n",
            "Epoch 10 Batch 300 Loss 5.1067\n",
            "Epoch 10 Loss 4.0426\n",
            "Time taken for 1 epoch 25.428818464279175 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbQpyYs13jF_"
      },
      "source": [
        "def evaluate_sentence(sentence):\n",
        "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "  result = ''\n",
        "\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
        "  end_token = targ_lang.word_index['<end>']\n",
        "\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "  # Instantiate BasicDecoder object\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
        "  # Setup Memory in decoder stack\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\n",
        "\n",
        "  # set decoder_initial_state\n",
        "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
        "\n",
        "\n",
        "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "  \n",
        "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
        "  return outputs.sample_id.numpy()\n",
        "\n",
        "def translate(sentence):\n",
        "  result = evaluate_sentence(sentence)\n",
        "  print(result)\n",
        "  result = targ_lang.sequences_to_texts(result)\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJpT9D5_OgP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb821b2-c410-433e-848f-10a0d072c0fd"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f750c8909d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYmYhNN_faR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8014b8d-d5a7-4d2b-ec07-fcf882a96fab"
      },
      "source": [
        "translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3]]\n",
            "Input: hace mucho frio aqui.\n",
            "Predicted translation: ['<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSx2iM36EZQZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366bf70d-ff10-4058-db8a-98899dd8aaef"
      },
      "source": [
        "translate(u'esta es mi vida.')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3]]\n",
            "Input: esta es mi vida.\n",
            "Predicted translation: ['<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3LLCx3ZE0Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538aae52-0935-4b7b-cc61-3be61c648f3b"
      },
      "source": [
        "translate(u'¿todavia estan en casa?')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3]]\n",
            "Input: ¿todavia estan en casa?\n",
            "Predicted translation: ['<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUQVLVqUE1YW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb525ba0-2ecc-4049-8ef8-8b1f633b096b"
      },
      "source": [
        "# wrong translation\n",
        "translate(u'trata de averiguarlo.')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3]]\n",
            "Input: trata de averiguarlo.\n",
            "Predicted translation: ['<end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJ-RTQ0hsJNL"
      },
      "source": [
        "def beam_evaluate_sentence(sentence, beam_width=3):\n",
        "  sentence = dataset_creator.preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "  result = ''\n",
        "\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
        "  end_token = targ_lang.word_index['<end>']\n",
        "\n",
        "  # From official documentation\n",
        "  # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
        "  # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
        "  # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
        "  # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
        "\n",
        "  enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\n",
        "  print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
        "\n",
        "  # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "  hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
        "  decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
        "  decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
        "\n",
        "  # Instantiate BeamSearchDecoder\n",
        "  decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "  # The BeamSearchDecoder object's call() function takes care of everything.\n",
        "  outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
        "  # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
        "  # The final beam predictions are stored in outputs.predicted_id\n",
        "  # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
        "  # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
        "  # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
        "\n",
        "  \n",
        "  # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
        "  # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
        "  # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
        "  final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
        "  beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
        "  \n",
        "  return final_outputs.numpy(), beam_scores.numpy()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_LvXGvX8X-O"
      },
      "source": [
        "def beam_translate(sentence):\n",
        "  result, beam_scores = beam_evaluate_sentence(sentence)\n",
        "  print(result.shape, beam_scores.shape)\n",
        "  for beam, score in zip(result, beam_scores):\n",
        "    print(beam.shape, score.shape)\n",
        "    output = targ_lang.sequences_to_texts(beam)\n",
        "    output = [a[:a.index('<end>')] for a in output]\n",
        "    beam_score = [a.sum() for a in score]\n",
        "    print('Input: %s' % (sentence))\n",
        "    for i in range(len(output)):\n",
        "      print('{} Predicted translation: {}  {}'.format(i+1, output[i], beam_score[i]))\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TODnXBleDzzO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06059be1-fa6d-496b-f3e7-ebf33989f306"
      },
      "source": [
        "beam_translate(u'hace mucho frio aqui.')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 16, 1024)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/resource_loader.py:103: UserWarning: You are currently using TensorFlow 2.4.1 and trying to load a custom op (custom_ops/seq2seq/_beam_search_ops.so).\n",
            "TensorFlow Addons has compiled its custom ops against TensorFlow 2.2.0, and there are no compatibility guarantees between the two versions. \n",
            "This means that you might get segfaults when loading the custom op, or other kind of low-level errors.\n",
            " If you do, do not file an issue on Github. This is a known limitation.\n",
            "\n",
            "It might help you to fallback to pure Python ops with TF_ADDONS_PY_OPS . To do that, see https://github.com/tensorflow/addons#gpucpu-custom-ops \n",
            "\n",
            "You can also change the TensorFlow version installed on your system. You would need a TensorFlow version equal to or above 2.2.0 and strictly below 2.3.0.\n",
            " Note that nightly versions of TensorFlow, as well as non-pip TensorFlow like `conda install tensorflow` or compiled from source are not supported.\n",
            "\n",
            "The last solution is to find the TensorFlow Addons version that has custom ops compatible with the TensorFlow installed on your system. To do that, refer to the readme: https://github.com/tensorflow/addons\n",
            "  UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/options.py:47: RuntimeWarning: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/seq2seq/beam_search_decoder.py\", line 239, in gather_tree\n",
            "    return _beam_search_so.ops.addons_gather_tree(\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/resource_loader.py\", line 64, in ops\n",
            "    self._ops = tf.load_op_library(get_path_to_datafile(self.relative_path))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/load_library.py\", line 57, in load_op_library\n",
            "    lib_handle = py_tf.TF_LoadLibrary(library_filename)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.7/dist-packages/tensorflow_addons/custom_ops/seq2seq/_beam_search_ops.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb\n",
            "\n",
            "\n",
            "The gather_tree C++/CUDA custom op could not be loaded.\n",
            "For this reason, Addons will fallback to an implementation written\n",
            "in Python with public TensorFlow ops. There worst you might experience with\n",
            "this is a moderate slowdown on GPU. There can be multiple\n",
            "reason for this loading error, one of them may be an ABI incompatibility between\n",
            "the TensorFlow installed on your system and the TensorFlow used to compile\n",
            "TensorFlow Addons' custom ops. The stacktrace generated when loading the\n",
            "shared object file was displayed above.\n",
            "\n",
            "If you want this warning to disappear, either make sure the TensorFlow installed\n",
            "is compatible with this version of Addons, or tell TensorFlow Addons to\n",
            "prefer using Python implementations and not custom C++/CUDA ones. You can do that\n",
            "by changing the TF_ADDONS_PY_OPS flag\n",
            "either with the environment variable:\n",
            "```bash\n",
            "TF_ADDONS_PY_OPS=1 python my_script.py\n",
            "```\n",
            "or in your code, after your imports:\n",
            "```python\n",
            "import tensorflow_addons as tfa\n",
            "import ...\n",
            "import ...\n",
            "\n",
            "tfa.options.TF_ADDONS_PY_OPS = True\n",
            "```\n",
            "\n",
            "  warnings.warn(warning_msg, RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 3, 3) (1, 3, 3)\n",
            "(3, 3) (3, 3)\n",
            "Input: hace mucho frio aqui.\n",
            "1 Predicted translation:   -24.939064025878906\n",
            "2 Predicted translation: .   -41.613136291503906\n",
            "3 Predicted translation: . .   -50.06826400756836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BezQwENFY3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d63beb8-3309-41fb-ad65-e9a719d57603"
      },
      "source": [
        "beam_translate(u'¿todavia estan en casa?')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 16, 1024)\n",
            "(1, 3, 3) (1, 3, 3)\n",
            "(3, 3) (3, 3)\n",
            "Input: ¿todavia estan en casa?\n",
            "1 Predicted translation:   -24.955049514770508\n",
            "2 Predicted translation: .   -41.64100646972656\n",
            "3 Predicted translation: . .   -50.09756088256836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si68rj5homcO"
      },
      "source": [
        ""
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}