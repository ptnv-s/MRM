{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (2.0.0b1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.35.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (1.14.0a20190603)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: h5py in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.10.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (51.3.3.post20210118)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: importlib-metadata in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /anaconda/envs/py37_tensorflow/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow) (3.4.0)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "su00atkqqzjZ",
    "outputId": "ada55cad-f86a-4919-bc39-a746d0b0b501"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IF3LjvXqdJi"
   },
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "cUoGvxNddFHh",
    "outputId": "7e1b86b4-0462-4653-fb0c-35b3719f760d"
   },
   "outputs": [],
   "source": [
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s): \n",
    "    \n",
    "    # initialize an empty string\n",
    "    str1 = ' ' \n",
    "    \n",
    "    # return string  \n",
    "    return (str1.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "44g54wKWqdJp",
    "outputId": "112f25af-1c1d-4e1a-9869-e5ee602d5366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "keras = tf.keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "DEed-KleNdUK",
    "outputId": "4a4373cb-9ab9-4fc5-b654-b5bb8b6cbd65"
   },
   "outputs": [],
   "source": [
    "!wget https://www.manythings.org/anki/fra-eng.zip\n",
    "!unzip  fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UHgfCZXVqdJu"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSK6zPRUqdJz"
   },
   "outputs": [],
   "source": [
    "class Lang(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2int = {}\n",
    "        self.word2count = {}\n",
    "        self.int2word = {0 : \"SOS\", 1 : \"EOS\"}\n",
    "        self.n_words = 2\n",
    "        \n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2int:\n",
    "            self.word2int[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.int2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "            \n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    \n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UujikttqdJ7"
   },
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) \\\n",
    "                   if unicodedata.category(c) != \"Mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sb0sLusAqdKA"
   },
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    \n",
    "    s = re.sub(r\"([!.?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z?.!]+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mbvFe0iqqdKF"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    with open(\"fra.txt\",'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    pairs = [[normalizeString(pair) for pair in \n",
    "              line.strip().split('\\t')] for line in lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26fM5LRUqdKJ"
   },
   "outputs": [],
   "source": [
    "pairs = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnmGqtDOqdKN"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n",
    "def sentencetoIndexes(sentence, lang):\n",
    "    indexes = [lang.word2int[word] for word in sentence.split()]\n",
    "    indexes.append(EOS_token)\n",
    "    return indexes\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split()) < MAX_LENGTH and \\\n",
    "len(p[1].split()) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "pairs = filterPairs(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GQbB37mcsHb_",
    "outputId": "fad2916c-973b-49e3-9543-ca0dbefa43ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go .', 'va !', 'cc by . france attribution tatoeba .org cm wittydev ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlAGPZShqdKT"
   },
   "outputs": [],
   "source": [
    "def build_lang(lang1, lang2, max_length=10):\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    input_seq = []\n",
    "    output_seq = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[1])\n",
    "        output_lang.addSentence(pair[0])\n",
    "    for pair in pairs:\n",
    "        input_seq.append(sentencetoIndexes(pair[1], input_lang))\n",
    "        output_seq.append(sentencetoIndexes(pair[0], output_lang))\n",
    "    return keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_length, padding='post',\n",
    "                                                      truncating='post'), \\\n",
    "keras.preprocessing.sequence.pad_sequences(output_seq, padding='post', truncating='post'), input_lang, output_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9MAxjU-qdKY"
   },
   "outputs": [],
   "source": [
    "input_tensor, output_tensor, input_lang, output_lang = build_lang('fr', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YZ8Pcm1Ls8Cc",
    "outputId": "2937839a-ccbd-4eba-da49-85f03e91bdbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fr'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_lang.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qii6d6iKqdKc"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 80\n",
    "BUFFER_SIZE = len(input_tensor)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41nSXuflqdKh"
   },
   "outputs": [],
   "source": [
    "class Encoder(keras.models.Model):\n",
    "    def __init__(self, vocab_size, num_hidden=1000, num_embedding=1000, batch_size=80):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_embedding = num_embedding\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, num_embedding)\n",
    "        self.gru = keras.layers.GRU(num_hidden, return_sequences=True,\n",
    "                                    recurrent_initializer='glorot_uniform',\n",
    "                                   return_state=True)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        rnn_out, hidden = self.gru(embedded, initial_state=hidden)\n",
    "        return rnn_out, hidden\n",
    "    def init_hidden(self):\n",
    "        return tf.zeros(shape=(self.batch_size, self.num_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qm8zLgoqdKk"
   },
   "outputs": [],
   "source": [
    "inputs, outputs = next(iter(dataset))\n",
    "hidden = tf.zeros((80, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5AHvxMiqdKo"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCF-S8fBqdKs"
   },
   "outputs": [],
   "source": [
    "e_outputs, e_hidden = encoder(inputs, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "AqGefguVqdKv",
    "outputId": "0b8b976b-6267-40b2-be27-f663781cf1f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80, 1000), dtype=float32, numpy=\n",
       "array([[-3.29989917e-03, -9.93447751e-03,  1.99688673e-02, ...,\n",
       "        -9.54078510e-03, -4.48483601e-02, -1.99051984e-02],\n",
       "       [-8.53081886e-03, -9.39708389e-03, -1.07009578e-02, ...,\n",
       "        -1.28178280e-02,  1.60228051e-02,  9.20788758e-03],\n",
       "       [ 6.66846056e-03, -8.90456140e-05, -1.02417916e-03, ...,\n",
       "        -1.70747412e-03,  1.71073247e-02,  1.51243880e-02],\n",
       "       ...,\n",
       "       [-3.91800888e-03, -1.01863239e-02,  1.51172793e-02, ...,\n",
       "        -8.07212293e-03, -3.38810198e-02, -1.45891150e-02],\n",
       "       [-1.97292469e-03, -1.44110750e-02,  1.64883323e-02, ...,\n",
       "        -1.10460855e-02, -3.48327123e-02, -1.22437635e-02],\n",
       "       [ 1.58518441e-02,  1.43838376e-02,  1.02042337e-03, ...,\n",
       "         5.99017832e-03, -8.31275806e-03,  5.87579422e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a131eff28358>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoder.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36_tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2232\u001b[0m             f, self.layers, skip_mismatch=skip_mismatch)\n\u001b[1;32m   2233\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2234\u001b[0;31m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2236\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36_tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    660\u001b[0m   \"\"\"\n\u001b[1;32m    661\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m'keras_version'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m     \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keras_version'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0moriginal_keras_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "encoder.load_weights('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_bJHJsq1qdK6"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(keras.models.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "    \n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, encoder_out, hidden):\n",
    "        #shape of encoder_out : batch_size, seq_length, hidden_dim \n",
    "        #shape of encoder_hidden : batch_size, hidden_dim \n",
    "        \n",
    "        hidden = tf.expand_dims(hidden, axis=1) #out:\n",
    "        \n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_out) + \\\n",
    "                                  self.W2(hidden))) \n",
    "        \n",
    "        attn_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        context =  attn_weights * encoder_out \n",
    "        context = tf.reduce_sum(context, axis=1)\n",
    "        return context, attn_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gRC60cCqdK-"
   },
   "outputs": [],
   "source": [
    "attn = BahdanauAttention(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHU6rIRzqdLC"
   },
   "outputs": [],
   "source": [
    "context, attn_weights = attn(e_outputs, e_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G2XacZYkqdLK",
    "outputId": "2f5997bb-f3bd-4679-e12a-70f2181bfbe6"
   },
   "outputs": [],
   "source": [
    "attn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsLImrCwqdLT"
   },
   "outputs": [],
   "source": [
    "class Decoder(keras.models.Model):\n",
    "    def __init__(self, vocab_size, dec_dim=1000, embedding_dim=1000):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.attn = BahdanauAttention(dec_dim)\n",
    "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(dec_dim, recurrent_initializer='glorot_uniform',\n",
    "                                   return_sequences=True, return_state=True)\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, enc_hidden, enc_out):\n",
    "        x = self.embedding(x)\n",
    "        context, attn_weights = self.attn(enc_out, enc_hidden)\n",
    "        x = tf.concat((tf.expand_dims(context, 1), x), -1)\n",
    "        r_out, hidden = self.gru(x, initial_state=enc_hidden)\n",
    "        out = tf.reshape(r_out,shape=(-1, r_out.shape[2]))\n",
    "        return self.fc(out), hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ItYHAqWJqdLY"
   },
   "outputs": [],
   "source": [
    "decoder = Decoder(output_lang.n_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnhNtfb9qdLe"
   },
   "outputs": [],
   "source": [
    "input_tensor, output_tensor = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G7Y7QsykqdLi"
   },
   "outputs": [],
   "source": [
    "x = np.expand_dims(output_tensor[:,1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DE7fnvAqdLp"
   },
   "outputs": [],
   "source": [
    "def loss_fn(real, pred):\n",
    "    criterion = keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                           reduction='none')\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    _loss = criterion(real, pred)\n",
    "    mask = tf.cast(mask, dtype=_loss.dtype)\n",
    "    _loss *= mask\n",
    "    return tf.reduce_mean(_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMc7zD6WqdLt"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adadelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hiAmynEsqdLw"
   },
   "outputs": [],
   "source": [
    "def train_step(input_tensor, target_tensor, enc_hidden):\n",
    "    loss = 0.0\n",
    "    with tf.GradientTape() as tape:\n",
    "    \n",
    "        batch_size = input_tensor.shape[0]\n",
    "        enc_output, enc_hidden = encoder(input_tensor, enc_hidden)\n",
    "\n",
    "        SOS_tensor = np.array([SOS_token])\n",
    "        dec_input = tf.squeeze(tf.expand_dims([SOS_tensor]*batch_size, 1), -1)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        for tx in range(target_tensor.shape[1]-1):\n",
    "          \n",
    "            dec_out, dec_hidden, _ = decoder(dec_input, dec_hidden,\n",
    "                                            enc_output)\n",
    "            loss += loss_fn(target_tensor[:, tx], dec_out)\n",
    "            dec_input = tf.expand_dims(target_tensor[:, tx], 1)\n",
    "\n",
    "    batch_loss = loss / target_tensor.shape[1]\n",
    "    t_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, t_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, t_variables))\n",
    "    return batch_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6kj6diJMqdL0",
    "outputId": "97f30803-0152-4e1a-dee9-92d68251ec4e"
   },
   "outputs": [],
   "source": [
    "hidden = tf.zeros(shape=(80, 1000))\n",
    "loss = train_step(input_tensor, output_tensor, hidden)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_weights('encoder.h5')\n",
    "decoder.load_weights('decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lk_OG391qdL5"
   },
   "outputs": [],
   "source": [
    "def checkpoint(model, name=None):\n",
    "    if name is not None:\n",
    "        model.save_weights('{}.h5'.format(name))\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZEbkwx9qdL-"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "log_every = 50\n",
    "steps_per_epoch = len(pairs) // BATCH_SIZE\n",
    "loss_list = []\n",
    "\n",
    "for e in range(1, EPOCHS):\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    enc_hidden = encoder.init_hidden()\n",
    "    \n",
    "    for idx, (input_tensor, target_tensor) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input_tensor, target_tensor, hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if idx % log_every == 0:\n",
    "            loss_list.append(batch_loss)\n",
    "            print(\"Epochs: {} batch_loss: {:.4f}\".format(e, batch_loss))\n",
    "            checkpoint(encoder, 'encoder')\n",
    "            checkpoint(decoder, 'decoder')\n",
    "            \n",
    "    if e % 2 == 0:\n",
    "        print(\"Epochs: {}/{} total_loss: {:.4f}\".format(\n",
    "        e, EPOCHS, total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JuswAWMYqdME"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, max_length=10):\n",
    "    result = ''\n",
    "    attention_plot = np.zeros((10,10))\n",
    "    sentence = normalizeString(sentence)\n",
    "    sentence = sentencetoIndexes(sentence, input_lang)\n",
    "    sentence = keras.preprocessing.sequence.pad_sequences([sentence],padding='post',\n",
    "                                                      maxlen=max_length, truncating='post')\n",
    "    \n",
    "    encoder_hidden = hidden = [tf.zeros((1, 256))]\n",
    "    \n",
    "    enc_out, enc_hidden = encoder(sentence, encoder_hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    SOS_tensor = np.array([SOS_token])\n",
    "    dec_input = tf.squeeze(tf.expand_dims([SOS_tensor], 1), -1)\n",
    "    \n",
    "    for tx in range(max_length):\n",
    "        dec_out, dec_hidden, attn_weights = decoder(dec_input,\n",
    "                                                   dec_hidden, enc_out)\n",
    "        attn_weights = tf.reshape(attn_weights, (-1, ))\n",
    "        attention_plot[tx] = attn_weights.numpy()\n",
    "        pred = tf.argmax(dec_out, axis=1).numpy()\n",
    "        result += output_lang.int2word[pred[0]] + \" \"\n",
    "        if output_lang.int2word[pred[0]] == \"EOS\":\n",
    "            break\n",
    "        dec_input = tf.expand_dims(pred, axis=1)\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "r7mmzYUIqdMI",
    "outputId": "e78bcf6c-9cea-4d8a-880a-27a4acabc95e"
   },
   "outputs": [],
   "source": [
    "sentence = \"j'ai besoin de quelqu'un pour m'aider ?\"\n",
    "pred, attn_weights = translate(sentence)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-s9cnSvUxNIC"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    sentence = normalizeString(listToString(sentence))\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + Convert(sentence), fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpf4T5z_3JP4"
   },
   "outputs": [],
   "source": [
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "5A9xpAf6xsgL",
    "outputId": "fa47f721-b6e9-4a46-84e2-e402738e7bdc"
   },
   "outputs": [],
   "source": [
    "attn_weights = attn_weights[:len(pred.split(' ')), :len(sentence.split(' '))]\n",
    "plot_attention(attn_weights, sentence.split(), pred.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yROfA7newSua"
   },
   "outputs": [],
   "source": [
    "keras.model.load('encoder.h5')\n",
    "keras.model.load('decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5vDXB800z9GC"
   },
   "outputs": [],
   "source": [
    "pretrained_model = create_functional_model()\n",
    "pretrained_model.load_weights(\"pretrained_weights.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "machine translation tensorflow orig.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "azureml_py36_tensorflow",
   "language": "python",
   "name": "conda-env-azureml_py36_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
